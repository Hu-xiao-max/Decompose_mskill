#!/usr/bin/env python3
"""
ä¸å¸¦ä»¿çœŸè¯„ä¼°çš„Diffusion Policyè®­ç»ƒè„šæœ¬
"""

import os
import json
import argparse
import torch
import torch.nn as nn
from torch import optim
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm
import numpy as np
from typing import Dict
import time
from datetime import datetime

# è®¾ç½®æ— å¤´æ¨¡å¼ï¼Œç¡®ä¿æ²¡æœ‰GUIä¾èµ–
os.environ['QT_QPA_PLATFORM'] = 'minimal'
os.environ['MPLBACKEND'] = 'Agg'
os.environ.pop('DISPLAY', None)

from data_loader import create_data_loaders
from diffusion_model import create_diffusion_policy


class SimpleTrainer:
    """ç®€å•è®­ç»ƒå™¨ï¼ˆæ— ä»¿çœŸè¯„ä¼°ï¼‰"""
    
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        device: torch.device,
        config: Dict
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.config = config
        
        # ä¼˜åŒ–å™¨ - ä½¿ç”¨Adamï¼Œå› ä¸ºAdamWå¯èƒ½ä¸å¯ç”¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
            betas=config.get('betas', (0.9, 0.999)),
            eps=config.get('eps', 1e-8)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config['num_epochs'],
            eta_min=config['learning_rate'] * 0.01
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = config.get('use_amp', True) and torch.cuda.is_available()
        if self.use_amp:
            self.scaler = GradScaler()
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.global_step = 0
        self.best_val_loss = float('inf')
        self.train_losses = []
        self.val_losses = []
        
        # æ—©åœæœºåˆ¶
        self.patience = config.get('patience', 20)
        self.patience_counter = 0
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = config['save_dir']
        os.makedirs(self.save_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        with open(os.path.join(self.save_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)
    
    def compute_loss(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """è®¡ç®—æŸå¤±"""
        images = batch['images'].to(self.device)
        robot_states = batch['robot_states'].to(self.device)
        actions = batch['actions'].to(self.device)
        
        batch_size = actions.shape[0]
        
        # éšæœºé‡‡æ ·æ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.model.num_diffusion_steps, 
            (batch_size,), device=self.device
        ).long()
        
        # ä¸ºåŠ¨ä½œæ·»åŠ å™ªå£°
        noisy_actions, noise = self.model.add_noise(actions, timesteps)
        
        # å‰å‘ä¼ æ’­
        if self.use_amp:
            with autocast():
                predicted_noise = self.model(noisy_actions, timesteps, images, robot_states)
                mse_loss = nn.functional.mse_loss(predicted_noise, noise)
        else:
            predicted_noise = self.model(noisy_actions, timesteps, images, robot_states)
            mse_loss = nn.functional.mse_loss(predicted_noise, noise)
        
        losses = {'mse_loss': mse_loss, 'total_loss': mse_loss}
        return losses
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {self.epoch + 1}")
        
        for batch_idx, batch in enumerate(pbar):
            self.optimizer.zero_grad()
            
            try:
                losses = self.compute_loss(batch)
                total_loss = losses['total_loss']
                
                # åå‘ä¼ æ’­
                if self.use_amp:
                    self.scaler.scale(total_loss).backward()
                    if self.config.get('grad_clip_norm', 0) > 0:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(
                            self.model.parameters(), 
                            self.config['grad_clip_norm']
                        )
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    total_loss.backward()
                    if self.config.get('grad_clip_norm', 0) > 0:
                        torch.nn.utils.clip_grad_norm_(
                            self.model.parameters(), 
                            self.config['grad_clip_norm']
                        )
                    self.optimizer.step()
                
                epoch_loss += total_loss.item()
                num_batches += 1
                self.global_step += 1
                
                pbar.set_postfix({
                    'loss': f"{total_loss.item():.4f}",
                    'lr': f"{self.optimizer.param_groups[0]['lr']:.6f}"
                })
                
                # é™åˆ¶è®­ç»ƒæ‰¹æ¬¡æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
                if (self.config.get('max_train_batches', 0) > 0 and 
                    batch_idx >= self.config['max_train_batches']):
                    break
                    
            except Exception as e:
                print(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue
        
        avg_loss = epoch_loss / max(num_batches, 1)
        return {'train_loss': avg_loss}
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯"""
        self.model.eval()
        val_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(self.val_loader, desc="Validation")):
                try:
                    losses = self.compute_loss(batch)
                    val_loss += losses['total_loss'].item()
                    num_batches += 1
                    
                    if (self.config.get('max_val_batches', 0) > 0 and 
                        batch_idx >= self.config['max_val_batches']):
                        break
                        
                except Exception as e:
                    print(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                    continue
        
        avg_loss = val_loss / max(num_batches, 1)
        return {'val_loss': avg_loss}
    
    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_val_loss': self.best_val_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'config': self.config
        }
        
        if self.use_amp:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, os.path.join(self.save_dir, 'latest_checkpoint.pth'))
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save(checkpoint, os.path.join(self.save_dir, 'best_model.pth'))
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒæŸå¤±: {self.best_val_loss:.4f}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("=" * 60)
        print("å¼€å§‹è®­ç»ƒ Diffusion Policy (æ— ä»¿çœŸè¯„ä¼°æ¨¡å¼)")
        print("=" * 60)
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"è®­ç»ƒé›†: {len(self.train_loader)} æ‰¹æ¬¡")
        print(f"éªŒè¯é›†: {len(self.val_loader)} æ‰¹æ¬¡")
        print("=" * 60)
        
        start_time = time.time()
        
        for epoch in range(self.epoch, self.config['num_epochs']):
            self.epoch = epoch
            epoch_start_time = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            self.train_losses.append(train_metrics['train_loss'])
            
            # éªŒè¯
            val_metrics = self.validate()
            self.val_losses.append(val_metrics['val_loss'])
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler is not None:
                self.scheduler.step()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = val_metrics['val_loss'] < self.best_val_loss
            
            if is_best:
                self.best_val_loss = val_metrics['val_loss']
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            # è®¡ç®—æ—¶é—´
            epoch_time = time.time() - epoch_start_time
            total_time = time.time() - start_time
            
            # æ‰“å°ç»“æœ
            print(f"\nEpoch {epoch + 1}/{self.config['num_epochs']} ({epoch_time:.1f}s)")
            print(f"  è®­ç»ƒæŸå¤±: {train_metrics['train_loss']:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_metrics['val_loss']:.4f}")
            print(f"  æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            print(f"  å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            print(f"  è€å¿ƒè®¡æ•°: {self.patience_counter}/{self.patience}")
            if is_best:
                print("  ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹!")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(is_best)
            
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= self.patience:
                print(f"\næ—©åœè§¦å‘! éªŒè¯æŸå¤±è¿ç»­ {self.patience} ä¸ªepochæ²¡æœ‰æ”¹å–„")
                break
            
            print("-" * 60)
        
        total_time = time.time() - start_time
        print(f"\nè®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time:.1f}s ({total_time/3600:.1f}h)")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")


def create_simple_config() -> Dict:
    """åˆ›å»ºç®€å•è®­ç»ƒé…ç½®"""
    return {
        # æ•°æ®é…ç½®
        # 'dataset_path': '/home/alien/Dataset/colosseum_dataset',
        'dataset_path': '/home/alien/simulation/robot-colosseum/dataset/close_box',
        'batch_size': 8,
        'sequence_length': 4,
        'action_horizon': 2,
        'num_workers': 0,
        'image_size': (224, 224),
        'normalize_actions': True,
        'augment_images': True,
        'cameras': ['front_rgb'],
        'image_types': ['rgb'],
        'load_depth': False,
        'load_point_clouds': False,
        'subsample_factor': 1,
        'max_episodes_per_task': None,
        
        # æ¨¡å‹é…ç½®
        'action_dim': 7,
        'state_dim': 15,  # å…³èŠ‚ä½ç½®(7) + å¤¹çˆªä½å§¿(7) + å¤¹çˆªçŠ¶æ€(1) = 15
        'vision_feature_dim': 256,
        'hidden_dim': 256,
        'num_diffusion_steps': 50,
        'num_layers': 2,
        'num_heads': 4,
        'dropout': 0.1,
        
        # è®­ç»ƒé…ç½®
        'num_epochs': 50,
        'learning_rate': 1e-4,
        'weight_decay': 1e-4,
        'grad_clip_norm': 1.0,
        'use_amp': True,
        'patience': 15,
        
        # ä¿å­˜é…ç½®
        'save_dir': '/home/alien/simulation/robot-colosseum/diffusion_policy/my_model',
        
        # è°ƒè¯•é…ç½®
        'max_train_batches': 0,  # 0è¡¨ç¤ºä¸é™åˆ¶
        'max_val_batches': 0,
    }


def main():
    parser = argparse.ArgumentParser(description='ç®€å•Diffusion Policyè®­ç»ƒï¼ˆæ— ä»¿çœŸè¯„ä¼°ï¼‰')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„ (JSON)')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--save_dir', type=str, default='./my_model', help='ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    if args.config:
        with open(args.config, 'r') as f:
            config = json.load(f)
    else:
        config = create_simple_config()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.epochs:
        config['num_epochs'] = args.epochs
    if args.batch_size:
        config['batch_size'] = args.batch_size
    if args.lr:
        config['learning_rate'] = args.lr
    if args.save_dir:
        config['save_dir'] = args.save_dir
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    try:
        train_loader, val_loader = create_data_loaders(
            dataset_path=config['dataset_path'],
            batch_size=config['batch_size'],
            sequence_length=config['sequence_length'],
            action_horizon=config['action_horizon'],
            num_workers=config['num_workers'],
            image_size=config['image_size'],
            normalize_actions=config['normalize_actions'],
            cameras=config['cameras'],
            image_types=config['image_types'],
            load_depth=config['load_depth'],
            load_point_clouds=config['load_point_clouds'],
            subsample_factor=config['subsample_factor'],
            max_episodes_per_task=config['max_episodes_per_task']
        )
    except Exception as e:
        print(f"åˆ›å»ºæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = create_diffusion_policy(
        action_dim=config['action_dim'],
        action_horizon=config['action_horizon'],
        state_dim=config['state_dim'],
        vision_feature_dim=config['vision_feature_dim'],
        hidden_dim=config['hidden_dim'],
        num_diffusion_steps=config['num_diffusion_steps'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        dropout=config['dropout']
    )
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SimpleTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        config=config
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()



if __name__ == "__main__":
    main()
